"""
Dcard Authenticated Crawler
ä½¿ç”¨å·²å„²å­˜çš„èªè­‰ç‹€æ…‹é€²è¡Œ Dcard çˆ¬å–
é¿å…ç™»å…¥æª¢æ¸¬å’Œåçˆ¬èŸ²é˜»æ“‹
"""
from __future__ import annotations

import asyncio
import json
import random
from pathlib import Path
from typing import List, Optional

from playwright.async_api import async_playwright, BrowserContext, Page

from .human_behavior import (
    HumanBehaviorSimulator,
    get_browser_context_options,
    simulate_mouse_movement,
    simulate_reading_pause,
    simulate_scroll,
)


class DcardCrawler:
    """
    Dcard å°ˆç”¨çˆ¬èŸ²
    ä½¿ç”¨å·²èªè­‰çš„ session ç‹€æ…‹
    """

    AUTH_STATE_PATH = Path(__file__).parent.parent.parent / "dcard-auth.json"

    def __init__(
        self,
        headless: bool = True,
        auth_state_path: Optional[str] = None,
    ):
        self.headless = headless
        self.auth_state_path = Path(auth_state_path) if auth_state_path else self.AUTH_STATE_PATH
        self._browser = None
        self._context: Optional[BrowserContext] = None
        self._playwright = None

    async def __aenter__(self):
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()

    async def start(self) -> None:
        """å•Ÿå‹•ç€è¦½å™¨ä¸¦è¼‰å…¥èªè­‰ç‹€æ…‹"""
        self._playwright = await async_playwright().start()
        self._browser = await self._playwright.chromium.launch(headless=self.headless)

        # æº–å‚™ context è¨­å®š
        context_options = get_browser_context_options()

        # è¼‰å…¥å·²å„²å­˜çš„èªè­‰ç‹€æ…‹
        if self.auth_state_path.exists():
            context_options["storage_state"] = str(self.auth_state_path)
            print(f"âœ… å·²è¼‰å…¥èªè­‰ç‹€æ…‹: {self.auth_state_path}")
        else:
            print(f"âš ï¸ æ‰¾ä¸åˆ°èªè­‰ç‹€æ…‹æª”æ¡ˆ: {self.auth_state_path}")
            print("   è«‹å…ˆåŸ·è¡Œç™»å…¥ä¸¦å„²å­˜ç‹€æ…‹")

        self._context = await self._browser.new_context(**context_options)

    async def close(self) -> None:
        """é—œé–‰ç€è¦½å™¨"""
        if self._context:
            await self._context.close()
        if self._browser:
            await self._browser.close()
        if self._playwright:
            await self._playwright.stop()

    async def save_auth_state(self, path: Optional[str] = None) -> None:
        """å„²å­˜ç•¶å‰èªè­‰ç‹€æ…‹"""
        save_path = Path(path) if path else self.auth_state_path
        if self._context:
            state = await self._context.storage_state()
            save_path.write_text(json.dumps(state, ensure_ascii=False, indent=2))
            print(f"âœ… å·²å„²å­˜èªè­‰ç‹€æ…‹: {save_path}")

    async def fetch_forum_posts(
        self,
        forum: str = "talk",
        count: int = 20,
        sort: str = "popular",
    ) -> List[dict]:
        """
        æŠ“å–çœ‹æ¿æ–‡ç« åˆ—è¡¨

        Args:
            forum: çœ‹æ¿åç¨± (talk, mood, relationship, etc.)
            count: è¦æŠ“å–çš„æ–‡ç« æ•¸é‡
            sort: æ’åºæ–¹å¼ (popular, new)

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        if not self._context:
            raise RuntimeError("Crawler not started. Call start() first.")

        page = await self._context.new_page()
        human_sim = HumanBehaviorSimulator(page)
        await human_sim.warm_up()

        posts = []
        try:
            url = f"https://www.dcard.tw/f/{forum}"
            if sort == "new":
                url += "?tab=latest"

            # æ¨¡æ“¬äººé¡å°èˆª
            await simulate_mouse_movement(page, 0.5)
            await page.goto(url, wait_until="domcontentloaded")
            await simulate_reading_pause(1.0, 2.0)

            # æ²å‹•è¼‰å…¥æ›´å¤šæ–‡ç« 
            loaded_count = 0
            scroll_attempts = 0
            max_scrolls = count // 5 + 3

            while loaded_count < count and scroll_attempts < max_scrolls:
                await simulate_scroll(page, 2)
                await simulate_reading_pause(0.8, 1.5)

                # é€é API ç²å–æ–‡ç« è³‡æ–™
                articles = await page.query_selector_all('article')
                loaded_count = len(articles)
                scroll_attempts += 1

            # æå–æ–‡ç« è³‡è¨Š
            articles = await page.query_selector_all('article')
            for i, article in enumerate(articles[:count]):
                try:
                    # å–å¾—æ¨™é¡Œ
                    title_el = await article.query_selector('h2')
                    title = await title_el.inner_text() if title_el else ""

                    # å–å¾—é€£çµ
                    link_el = await article.query_selector('a[href*="/p/"]')
                    link = await link_el.get_attribute('href') if link_el else ""

                    # å–å¾—çœ‹æ¿
                    board_el = await article.query_selector('a[href^="/f/"]')
                    board = await board_el.inner_text() if board_el else forum

                    # å–å¾—æ‘˜è¦
                    summary_el = await article.query_selector('p')
                    summary = await summary_el.inner_text() if summary_el else ""

                    posts.append({
                        "index": i + 1,
                        "title": title.strip(),
                        "link": f"https://www.dcard.tw{link}" if link and not link.startswith('http') else link,
                        "board": board.strip(),
                        "summary": summary.strip()[:200],
                    })
                except Exception as e:
                    print(f"  âš ï¸ æ–‡ç«  {i+1} è§£æå¤±æ•—: {e}")

            print(f"âœ… æˆåŠŸæŠ“å– {len(posts)} ç¯‡æ–‡ç« ")

        except Exception as e:
            print(f"âŒ æŠ“å–å¤±æ•—: {e}")
        finally:
            await page.close()

        return posts

    async def fetch_post_content(self, post_url: str) -> dict:
        """
        æŠ“å–å–®ç¯‡æ–‡ç« å…§å®¹

        Args:
            post_url: æ–‡ç« ç¶²å€

        Returns:
            æ–‡ç« è©³ç´°å…§å®¹
        """
        if not self._context:
            raise RuntimeError("Crawler not started. Call start() first.")

        page = await self._context.new_page()
        human_sim = HumanBehaviorSimulator(page)
        await human_sim.warm_up()

        result = {
            "url": post_url,
            "title": "",
            "content": "",
            "author": "",
            "board": "",
            "reactions": {},
            "comments_count": 0,
        }

        try:
            # æ¨¡æ“¬äººé¡å°èˆª
            await simulate_mouse_movement(page, 0.5)
            await page.goto(post_url, wait_until="domcontentloaded")
            await simulate_reading_pause(1.5, 3.0)

            # æ¨¡æ“¬é–±è®€è¡Œç‚º
            await simulate_scroll(page, random.randint(2, 4))
            await simulate_mouse_movement(page, 1.0)

            # ç­‰å¾…å…§å®¹è¼‰å…¥
            await page.wait_for_load_state("networkidle")

            # æå–æ¨™é¡Œ
            title_el = await page.query_selector('h1')
            if title_el:
                result["title"] = await title_el.inner_text()

            # æå–å…§å®¹
            content_el = await page.query_selector('article')
            if content_el:
                result["content"] = await content_el.inner_text()

            # æå–ä½œè€…è³‡è¨Š
            author_el = await page.query_selector('a[href^="/@"]')
            if author_el:
                result["author"] = await author_el.inner_text()

            print(f"âœ… æˆåŠŸæŠ“å–æ–‡ç« : {result['title'][:50]}...")

        except Exception as e:
            print(f"âŒ æŠ“å–æ–‡ç« å¤±æ•—: {e}")
            result["error"] = str(e)
        finally:
            await page.close()

        return result

    async def search_posts(
        self,
        keyword: str,
        forum: Optional[str] = None,
        count: int = 20,
    ) -> List[dict]:
        """
        æœå°‹ Dcard æ–‡ç« 

        Args:
            keyword: æœå°‹é—œéµå­—
            forum: é™å®šçœ‹æ¿ (å¯é¸)
            count: è¦æŠ“å–çš„æ–‡ç« æ•¸é‡

        Returns:
            æœå°‹çµæœåˆ—è¡¨
        """
        if not self._context:
            raise RuntimeError("Crawler not started. Call start() first.")

        page = await self._context.new_page()
        human_sim = HumanBehaviorSimulator(page)
        await human_sim.warm_up()

        results = []
        try:
            # æ§‹å»ºæœå°‹ URL
            search_url = f"https://www.dcard.tw/search?query={keyword}"
            if forum:
                search_url += f"&forum={forum}"

            # æ¨¡æ“¬äººé¡å°èˆª
            await simulate_mouse_movement(page, 0.5)
            await page.goto(search_url, wait_until="domcontentloaded")
            await simulate_reading_pause(1.5, 2.5)

            # æ²å‹•è¼‰å…¥æ›´å¤šçµæœ
            for _ in range(count // 10 + 2):
                await simulate_scroll(page, 2)
                await simulate_reading_pause(0.8, 1.5)

            # æå–æœå°‹çµæœ
            articles = await page.query_selector_all('article')
            for i, article in enumerate(articles[:count]):
                try:
                    title_el = await article.query_selector('h2')
                    title = await title_el.inner_text() if title_el else ""

                    link_el = await article.query_selector('a[href*="/p/"]')
                    link = await link_el.get_attribute('href') if link_el else ""

                    summary_el = await article.query_selector('p')
                    summary = await summary_el.inner_text() if summary_el else ""

                    results.append({
                        "index": i + 1,
                        "title": title.strip(),
                        "link": f"https://www.dcard.tw{link}" if link and not link.startswith('http') else link,
                        "summary": summary.strip()[:200],
                    })
                except Exception as e:
                    print(f"  âš ï¸ çµæœ {i+1} è§£æå¤±æ•—: {e}")

            print(f"âœ… æœå°‹ '{keyword}' æ‰¾åˆ° {len(results)} ç¯‡æ–‡ç« ")

        except Exception as e:
            print(f"âŒ æœå°‹å¤±æ•—: {e}")
        finally:
            await page.close()

        return results


async def main():
    """æ¸¬è©¦ Dcard çˆ¬èŸ²"""
    async with DcardCrawler(headless=False) as crawler:
        # æ¸¬è©¦æŠ“å–ç†±é–€æ–‡ç« 
        print("\nğŸ“° æŠ“å–ç†±é–€æ–‡ç« ...")
        posts = await crawler.fetch_forum_posts("talk", count=10)
        for post in posts:
            print(f"  [{post['index']}] {post['title'][:40]}...")

        # æ¸¬è©¦æœå°‹
        print("\nğŸ” æœå°‹æ¸¬è©¦...")
        results = await crawler.search_posts("å·¥ä½œ", count=5)
        for r in results:
            print(f"  [{r['index']}] {r['title'][:40]}...")


if __name__ == "__main__":
    asyncio.run(main())
